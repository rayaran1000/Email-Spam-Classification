{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bcecb8",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; background-color: #649eff; color: white; padding: 14px; line-height: 1;border-radius:10px\">ðŸ“®EDA & Classification on Spam Email Dataset</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18153274",
   "metadata": {},
   "source": [
    "![image](https://thumbs.dreamstime.com/b/spam-mail-printed-wooden-cube-spam-mail-printed-wooden-cubes-193211215.jpg)\n",
    "\n",
    "<cite>Image source: https://www.dreamstime.com/photos-images/spam-mail.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c7010",
   "metadata": {},
   "source": [
    "> <h2> 1. About Dataset </h2>\n",
    "\n",
    "<br>\n",
    "The dataset <b>'Spam Email'</b> contains <b>2 columns</b>, each are:\n",
    "<br>\n",
    "\n",
    "* <b>Category</b>:     Whether it is spam or ham\n",
    "\n",
    "* <b>Message</b>:      context of message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7316870",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">1. Load Necessary Libraries and Dataset</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d0d0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7234c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Email Spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fea6e9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376c666",
   "metadata": {},
   "source": [
    "#### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a4e58e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category    0\n",
       "Message     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198efd9",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">2. Data Preprocessing</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbf8a20",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cc1c855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f9c0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "30d5691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_tokenized'] = df['Message'].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0ebb3",
   "metadata": {},
   "source": [
    "#### Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13f17813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "054e4d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "848541ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_non_punctuations'] = df['Message_tokenized'].apply(lambda x: [word for word in x if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e76f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing '...' and '..' from Message_non_puncutations\n",
    "dot_punctuations = ['..','...']\n",
    "df['Message_non_punctuations'] = df['Message_non_punctuations'].apply(lambda x : \" \".join([word for word in x if word not in dot_punctuations]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621ab94",
   "metadata": {},
   "source": [
    "#### Convertion to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78b79578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_lowercased'] = df['Message_non_punctuations'].apply(lambda x: \" \".join(word.lower() for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f2631",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">3. Text Analysis of the Data</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9f94c",
   "metadata": {},
   "source": [
    "#### As of now , will be considering Message_lowercased as the column for analysis after partial text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7b2cd",
   "metadata": {},
   "source": [
    "#### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6324dff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word_Count'] = df['Message_lowercased'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b22e0",
   "metadata": {},
   "source": [
    "#### Average Word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4b09fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3376\n",
      " 4824\n"
     ]
    }
   ],
   "source": [
    "for i,word in enumerate(df['Message_lowercased']):\n",
    "    if len(word) == 0:\n",
    "        print(word,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c8e8c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Message_tokenized</th>\n",
       "      <th>Message_non_punctuations</th>\n",
       "      <th>Message_lowercased</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>ham</td>\n",
       "      <td>:)</td>\n",
       "      <td>[:, )]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category Message Message_tokenized Message_non_punctuations  \\\n",
       "3376      ham      :)            [:, )]                            \n",
       "\n",
       "     Message_lowercased  Word_Count  \n",
       "3376                              0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3376:3377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e5e4fa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>Message_tokenized</th>\n",
       "      <th>Message_non_punctuations</th>\n",
       "      <th>Message_lowercased</th>\n",
       "      <th>Word_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>ham</td>\n",
       "      <td>:-) :-)</td>\n",
       "      <td>[:, -, ), :, -, )]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Category  Message   Message_tokenized Message_non_punctuations  \\\n",
       "4824      ham  :-) :-)  [:, -, ), :, -, )]                            \n",
       "\n",
       "     Message_lowercased  Word_Count  \n",
       "4824                              0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4824:4825]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1da510c",
   "metadata": {},
   "source": [
    "#### These 2 records will now be treated as null , because they dont give any value for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0aa6da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = (df['Word_Count'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15e1f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null_removed = df[~condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4a8c38e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageLength(x):\n",
    "    words = x.split()\n",
    "    return sum(len(word) for word in words) / len(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0193716",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average_Word_Length'] = df_null_removed['Message_lowercased'].apply(lambda x: AverageLength(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf057f",
   "metadata": {},
   "source": [
    "#### Stopwords Count and Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "257e7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd061ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "df['stopwords_count'] = df['Message_lowercased'].apply(lambda x: len([word for word in x.split() if word in stop_words]))\n",
    "df['stopwords_rate'] = df['stopwords_count'] / df['Word_Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8684d1a",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">4. Data Cleaning</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf3dcb4",
   "metadata": {},
   "source": [
    "#### Removing the null records permanently from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41f3db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~condition]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4dc66",
   "metadata": {},
   "source": [
    "#### Removing other punctuations that might be present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60a2fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arany\\AppData\\Local\\Temp\\ipykernel_1584\\3527112433.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['Message_punctuations_removed'] = df['Message_lowercased'].str.replace('[^\\w\\s]', '')\n"
     ]
    }
   ],
   "source": [
    "df['Message_punctuations_removed'] = df['Message_lowercased'].str.replace('[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43301d6c",
   "metadata": {},
   "source": [
    "#### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad87ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_stopwords_removed'] = df['Message_punctuations_removed'].apply(lambda x: \" \".join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66792fa",
   "metadata": {},
   "source": [
    "#### Converting Abbreviations from Top 50 recurring words to meaninful words manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4c6f25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u         1175\n",
       "call       577\n",
       "2          489\n",
       "ur         391\n",
       "get        387\n",
       "nt         381\n",
       "gt         318\n",
       "lt         316\n",
       "4          301\n",
       "ok         285\n",
       "go         282\n",
       "free       278\n",
       "know       262\n",
       "got        252\n",
       "like       246\n",
       "good       243\n",
       "come       232\n",
       "time       217\n",
       "day        211\n",
       "love       207\n",
       "want       194\n",
       "send       192\n",
       "text       189\n",
       "one        176\n",
       "Ã¼          173\n",
       "going      173\n",
       "txt        169\n",
       "need       167\n",
       "home       163\n",
       "lor        162\n",
       "r          161\n",
       "see        159\n",
       "sorry      159\n",
       "still      156\n",
       "stop       155\n",
       "today      153\n",
       "back       153\n",
       "dont       152\n",
       "n          151\n",
       "da         149\n",
       "reply      147\n",
       "hi         139\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "take       135\n",
       "later      135\n",
       "please     132\n",
       "think      132\n",
       "pls        126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(\" \".join(df['Message_stopwords_removed']).split()).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9154b",
   "metadata": {},
   "source": [
    "#### Converting the maximum recurring words to meaningful words manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c7e3db1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done on based on top 50 recurring words\n",
    "abbreviation_mapping = {\n",
    "    'u': 'you',\n",
    "    '2':'to',\n",
    "    'ur': 'your',\n",
    "    'n': 'and',\n",
    "    'gt': 'great',\n",
    "    'lt': 'little',\n",
    "    'nt':'not',\n",
    "    '4':'for',\n",
    "    'Ã¼':'you',\n",
    "    'txt':'text',\n",
    "    'r':'are',\n",
    "    'da':'the',\n",
    "    'pls':'please'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2821dbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveAbbreviations(x):\n",
    "    words = x.split()\n",
    "    updated_words = [abbreviation_mapping.get(word,word) for word in words]\n",
    "    return \" \".join(updated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0dce4bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_abreviations_treated'] = df['Message_stopwords_removed'].apply(RemoveAbbreviations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f360c",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "950eccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fad2180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm : To download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ada17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a7db5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([word.lemma_ for word in doc])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01af6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message_lemmatized'] = df['Message_abreviations_treated'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805610fe",
   "metadata": {},
   "source": [
    "#### Converting Spam to 1 and Ham to 0 (Target column encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bcff3c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Spam']=df['Category'].apply(lambda x:1 if x=='spam' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b33f2a",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"text-align: left; background-color: #78aaff; color: white; padding: 10px; line-height: 1;border-radius:20px\">5. Model Training and Evaluation</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "14d1a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Message_lemmatized']\n",
    "y = df['Spam']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92cdf8",
   "metadata": {},
   "source": [
    "#### Text to Vector conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "429d531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a88eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "cv_fit = cv.fit_transform(X)\n",
    "X_df = cv_fit.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489f1e7",
   "metadata": {},
   "source": [
    "#### Train and Test Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d0b36382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affcd947",
   "metadata": {},
   "source": [
    "#### Since the dataset is imbalanced , we are using stratify = y , to split the data equally between train and test based on distribution of target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "13fec587",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_df,y,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e3e25",
   "metadata": {},
   "source": [
    "#### Model Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e1b40f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73d639d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \n",
    "    'Logistic regressor' : LogisticRegression(),\n",
    "    'Multinomial Naive Bayes' : MultinomialNB(),\n",
    "    'Support Vector Classifier' : SVC()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983c8653",
   "metadata": {},
   "source": [
    "#### Evaluation metrics used for getting the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "080fdc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2282ef0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(models):\n",
    "    evaluate_report = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train,y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        roc_score = roc_auc_score(preds,y_test)\n",
    "        F1 = f1_score(preds,y_test)\n",
    "        evaluate_report[model_name] = {\n",
    "            'ROC_AUC score' : roc_score,\n",
    "            'F1 score' : F1\n",
    "        }\n",
    "        \n",
    "    return evaluate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6d3fd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = evaluate_model(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1f8c7cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic regressor': {'ROC_AUC score': 0.9908443540183113, 'F1 score': 0.9357142857142857}, 'Multinomial Naive Bayes': {'ROC_AUC score': 0.9435813854332145, 'F1 score': 0.9066666666666667}, 'Support Vector Classifier': {'ROC_AUC score': 0.9878665318503539, 'F1 score': 0.9124087591240876}}\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4a451",
   "metadata": {},
   "source": [
    "#### On checking the scores, it is evident that Logistic Regression is the best model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d325d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
